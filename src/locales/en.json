{
  "nav": {
    "home": "About",
    "education": "Education",
    "experience": "Experience",
    "projects": "Projects",
    "skills": "Skills",
    "resume": "Resume"
  },
  "footer": {
    "cv": "CV",
    "demo": "Demo",
    "code": "Code",
    "resume": "CV"
  },

  "ui": {
  "read_more": "Read more",
  "read_less": "Read less"
  },

  "hero": {
    "title": "Raphael Partouche",
    "subtitle": "Software Engineer and Master's student in Data Science at Eurecom. My background focuses on applying Machine Learning and Artificial Intelligence to develop solutions for a wide range of problems. My expertise includes the design, development, and deployment of intelligent algorithms, as well as the creation of data-driven software tools built upon a deep understanding of model performance and generalization challenges..."
  },
  "education": {
    "title": "Education",
    "eurecom": {
      "title": "EURECOM",
      "subtitle": "Engineering degree ‚Äî Master's in Data",
      "description": "High-level engineering program of excellence at the heart of the Sophia Antipolis technology hub, focused on data science and artificial intelligence. I am developing strong expertise in machine learning, databases, cloud computing, information systems security, and information theory.\n\n he curriculum includes <em><strong>Machine Learning, Database Systems, Quantum Computing, Cloud, Network security, Image processing, and Information theory</em></strong>, as well as cross-disciplinary subjects such as law and management.\n\n Moreover, the projects combine research and technical management, relying on regular analysis of scientific publications to guide innovative developments carried out within international teams, thus preparing for data, AI, or R&D engineering roles.",
      "year": "2024 ‚Äî 2027",
      "city": "Sophia Antipolis, France"
    },
    "jacques": {
      "title": "Lyc√©e Jacques-Decour",
      "subtitle": "Preparatory classes - PSCI/PSI*",
      "description": "Two years of intensive training in mathematics, physics, engineering sciences, and computer science within a highly selective and demanding academic environment, preparing for the national competitive exams to enter top French engineering schools. This program, grounded in scientific rigor, problem-solving, and logical reasoning, fosters strong analytical abilities, abstraction skills, and resilience under pressure. It provided me with solid theoretical foundations and an efficient working methodology, essential for tackling the technical and scientific challenges of engineering.",
      "year": "2022 ‚Äî 2024",
      "city": "Paris, France"
    }
  },
  "experience": {
    "title": "Experience",
    "civibot": {
      "title": "Software Engineer",
      "company": "CiviBot",
      "duration": "July 2025 ‚Äî Present",
      "location": "Hybrid",
      "description": "I joined CiviBot, a startup incubated at UC Berkeley SkyDeck, as a full-stack engineer contributing to the development of an AI-powered platform for civil engineers. My role combines frontend engineering in React with backend design in FastAPI, emphasizing clean architecture, scalability, and product reliability.\n\n On the frontend, I built and unified key user flows (authentication, subscription, account management) and implemented a full internationalization system using react-i18next. I also redesigned the navigation and form components to enhance UX consistency and code maintainability.\n\n On the backend, I developed and documented major REST endpoints (account management, contact, password reset/change) and contributed to the ChatPDF feature, integrating a document ingestion and vectorization pipeline to support contextualized Q&A.\n\n Since the end of the internship, I have continued working remotely with the CiviBot team, supporting the product‚Äôs technical growth and continuous integration."
    },
    "waterid": {
      "title": "Product Tester",
      "company": "Water ID",
      "duration": "July 2021 ",
      "location": "Karlsruhe, Germany",
      "description": "First international internship and initial engineering experience. Contributed to waterproofing and reliability testing of electronic devices designed to monitor swimming pool water quality. Involved in prototype validation and optimization of testing procedures within an industrial environment."
    }
  },
  "projects": {
    "title": "Projects",
    "civibot": {
    "title": "CiviBot",
    "description": "<strong>CiviBot ‚Äì Full Stack Engineering Project</strong>\n\nCiviBot is a Berkeley-based startup incubated at <strong>UC Berkeley SkyDeck</strong>, developing an AI assistant that helps engineers access and interpret technical standards. The platform integrates a full-stack web architecture (React + FastAPI) with a document-processing pipeline leveraging natural language models for contextual Q&A over uploaded PDFs.\n\nDuring this project, I contributed across the full software stack ‚Äî from frontend UI/UX to backend development and AI data handling ‚Äî to deliver a cohesive, production-ready experience.\n\n<strong>Frontend (React + Ant Design + i18next) </strong> \n\nI refactored and expanded the React web interface using Ant Design components for consistency and modularity. I implemented dynamic navigation based on authentication state (‚ÄúLogin/Sign Up‚Äù ‚Üí ‚ÄúMy Account/Logout‚Äù), standardized the footer across all pages, and developed the <em>About, Contact, Pricing, My Account, and Subscription pages.</em>  \nThe contact form was built with validation rules and backend-ready data structures. I also added a password recovery workflow (‚ÄúForgot password‚Äù) and a subscription flow with structured tier-based pricing.  \nI integrated full internationalization (FR/EN) using `react-i18next`, externalizing all UI text into JSON files for scalable multilingual support. This architectural choice ensures text coherence, maintainability, and readiness for future language expansion.\n\n<strong>Backend (FastAPI + PostgreSQL + Redis + GCS)</strong>  \n\n On the backend, I designed REST endpoints with clear data validation (Pydantic schemas), robust error handling, and modular separation between routes, schemas, and CRUD layers.  \nI developed endpoints for:  \n\n- Contact form submission (validated POST route + mail delivery) \n\n- Secure password update (hash verification + database update)  \n\n- Persistent chat sessions linked to documents (ChatPDF)\n\n <strong>The ChatPDF feature</strong> allows users to upload a PDF or image and query it in natural language. Each response is grounded in the original document and linked to exact source passages.Technically, the pipeline includes:  \n\n- File upload and storage on Google Cloud Storage (GCS)  \n\n- Document registration in PostgreSQL (metadata, GCS URL)  \n\n- Chat persistence (sessions, messages, PDF linkage)  \n\n- Rate limiting via Redis to prevent abuse  \n\n- Context reconstruction for multi-turn conversations\n\n AI Document Processing Pipeline, I contributed to the first stage of the document ingestion flow (upload ‚Üí parsing ‚Üí embedding ‚Üí indexing). The pipeline ensures that queries return traceable, source-grounded answers.\n\nThis project deepened my full-stack expertise, strengthened my understanding of API design, and taught me how to align UX, engineering, and data architecture in a fast-paced startup environment.\n\n <strong>Key Technical Skills : </strong><em>React, Ant Design, FastAPI, PostgreSQL,embedding, Vector Search, i18n</em>"
  }
  ,
    "othello": {
      "title": "Othello AI (Browser)",
      "description": "<strong>Othello AI ‚Äì Speech-Enabled Board Game</strong>\n\nThis academic project integrates <strong>AI logic</strong> and a <strong>modern web interface</strong> to create an interactive Othello game playable both online and by voice.\n\n<strong>Interface (React)</strong>\n\nThe interface, developed with React, offers a smooth, responsive design with seasonal themes for an immersive user experience (üåø‚ùÑüå∏‚òÄ). It ensures visual consistency and intuitive interactions across all platforms.\n\n<strong>AI System</strong>\n\nTwo AI models were implemented to enable strategic gameplay:\n\n- <em>Classic AI (Minimax Algorithm):</em> A rule-based approach that simulates possible moves to maximize advantage while minimizing the opponent‚Äôs options.\n\n- <em>Machine Learning AI (Random Forest Classifier):</em> Trained on real game data to predict optimal moves and continuously improve through self-learning.\n\nThis hybrid setup combines deterministic strategy with adaptive intelligence for performance comparison.\n\n<strong>Speech Recognition</strong>\n\nUsing <em>Python</em> and the <em>Google Speech Recognition API</em>, the system processes spoken commands through a band-pass filter to isolate speech frequencies and reduce noise. Recognized commands are then sent to the backend for action.<strong>\n\n"
    },
    "sightsense": {
      "title": "Sightsenses",
      "description": "<strong>SIGHT Senses ‚Äì Wearable Navigation Aid for the Visually Impaired</strong>\n\nSIGHT Senses is an academic engineering project focused on developing a <strong>wearable guidance system</strong> for visually impaired users. The device integrates <strong>computer vision</strong>, <strong>spatial audio</strong>, and <strong>haptic feedback</strong> into a unified architecture powered by a <em>Raspberry Pi</em>. It processes environmental information in real time to provide intuitive, multisensory navigation assistance.\n\n<strong>System Overview</strong>\n\nThe core system is structured around the Raspberry Pi, which synchronizes three technical modules: <em>image processing</em>, <em>sound spatialization</em>, and <em>haptic feedback</em>. Each module operates independently but exchanges data continuously to ensure real-time reactivity. A startup script (<code>crontab</code>) allows the device to launch automatically and operate autonomously once powered on.\n\n<strong>Image Processing (Python + OpenCV)</strong>\n\nThe vision module captures frames from the onboard camera and performs real-time <strong>shape and obstacle detection</strong>. Using OpenCV, it applies color-based segmentation (<code>cv2.inRange()</code>), Gaussian blur for noise reduction, and contour extraction (<code>cv2.findContours()</code>). It identifies <em>squares</em> and <em>crosses</em> via polygon approximation (<code>cv2.approxPolyDP()</code>) and geometric filtering (convexity, aspect ratio). Blue lines are also detected for path alignment. Once detected, the position and type of each element are transmitted to the feedback modules. For efficiency, only the most relevant shapes (largest or most centered) are kept, and vectorized OpenCV operations ensure fast computation.\n\n<strong>Distance and Obstacle Detection (Lidar)</strong>\n\nA <strong>TFLuna Lidar</strong> sensor continuously measures distances via serial communication (<code>/dev/serial0</code>). The readings are analyzed to detect obstacles and quantify proximity. Dynamic thresholds translate distance into feedback intensity, synchronizing the haptic and audio responses. This enables continuous awareness of surrounding obstacles, even in low-light or visually complex environments.\n\n<strong>Audio Processing (Python + PyDub)</strong>\n\nThe <strong>audio feedback system</strong> translates spatial data into directional cues. Built with PyDub, it adjusts stereo balance and timing to simulate a realistic 3D perception of the environment. The <code>balance_sound()</code> function modulates gain and delay between left and right channels depending on the obstacle‚Äôs position. Each type of detected object triggers a distinct preloaded sound (square, cross, line, fence), and the volume varies according to distance, offering clear and natural guidance through spatial hearing.\n\n<strong>Haptic Feedback (Python + RPi.GPIO)</strong>\n\nThe <strong>haptic subsystem</strong> complements the audio cues by converting distance data into tactile signals. Controlled via PWM on the Raspberry Pi GPIO, the <code>vibrate()</code> and <code>haptic_()</code> functions generate vibrations of variable frequency and intensity. The closer the obstacle, the stronger and more intermittent the vibration, providing an intuitive sense of spatial proximity. This dual-sensory redundancy (audio + touch) ensures reliable feedback in noisy or distracting environments.\n\n<strong>Hardware Integration and 3D Design</strong>\n\nThe device‚Äôs housing was modeled in SolidWorks for ergonomic and modular assembly. It contains the Raspberry Pi, Lidar, camera, battery, and vibration motors in a compact chest-mounted unit. Ventilation channels prevent overheating, while adjustable straps ensure user comfort and correct sensor alignment. The modular structure allows easy replacement and maintenance of components.\n\nThis project demonstrates the seamless integration of <strong>AI-driven perception</strong>, <strong>signal processing</strong>, and <strong>embedded systems</strong> within a wearable navigation device. The system operates autonomously, synchronizing image, sound, and haptic feedback in real time to deliver intuitive and reliable guidance for visually impaired individuals.\n\n"
    },
    "ui": {
      "previous": "Previous project",
      "next": "Next project",
      "close": "Close",
      "previous_image": "Previous image",
      "next_image": "Next image",
      "visit_site": "Visit site",
      "view_repo": "View repository"
    }
  },
  "skills": {
    "title": "Skills"
  }
}
