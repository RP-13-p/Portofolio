{
  "nav": {
    "home": "Profil",
    "education": "Formation",
    "experience": "Expérience",
    "projects": "Projets",
    "contact": "Contact",
    "skills": "Compétences",
    "resume": "CV"
  },

  "ui": {
  "read_more": "Lire plus",
  "read_less": "Lire moins"
  },

  "footer": {
    "cv": "CV",
    "demo": "Démo",
    "code": "Code",
    "resume": "CV"
  },
  "hero": {
    "title": "Raphael Partouche",
    "subtitle": "Ingénieur Software et étudiant en Master Data Science à Eurecom. Mon parcours se concentre sur l’application du Machine Learning et de l’Intelligence Artificielle à la création de solutions à divers problèmes. Mon expertise englobe la conception, le développement et le déploiement d’algorithmes intelligents ainsi que la conception d’outils logiciels exploitant la donnée et reposant sur une compréhension approfondie des enjeux liés à la performance et à la généralisation des modèles..."
  },
  "about": {
    "title": "",
    "text": ""
  },
  "education": {
    "title": "Formation",
    "eurecom": {
      "title": "EURECOM",
      "subtitle": "Diplôme d’ingénieur — Master en Data",
      "description": "Formation d’ingénieur d’excellence au cœur du pôle technologique de Sophia Antipolis, orientée vers la science des données et l’intelligence artificielle. J’y développe une expertise solide en apprentissage automatique, bases de données, Cloud, sécurité des systèmes d’information et théorie de l’information.\n\n Les enseignements incluent <em><strong> Machine Learning, Database Systems, Quantum Computing, Cloud, Network Security, Image Processing, Information Theory</strong></em>, mais aussi des disciplines transversales telles que le droit et le management.\n\n De plus, les projets combinent recherche et gestion technique, s’appuyant sur l’analyse régulière d’articles scientifiques pour guider des développements innovants menés en équipe internationale, formant ainsi à des postes d’ingénieur data, IA.",
      "year": "2024 — 2027",
      "city": "Sophia Antipolis, France"
    },
    "jacques": {
      "title": "Lycée Jacques-Decour",
      "subtitle": "Classes préparatoires - PSCI/PSI*",
      "description": "Deux années de formation intensive en mathématiques, physique, sciences de l’ingénieur et informatique, au sein d’un environnement académique sélectif et exigeant préparant aux concours d’entrée des grandes écoles d’ingénieurs françaises. Cette formation, fondée sur la rigueur scientifique, la résolution de problèmes complexes et la maîtrise des raisonnements logiques, développe une solide capacité d’abstraction, d’analyse et de travail sous pression. Elle m’a apporté des bases théoriques robustes et une méthode de travail efficace, essentielles pour aborder les défis techniques et scientifiques rencontrés dans le domaine de l’ingénierie.",
      "year": "2022 — 2024",
      "city": "Paris, France"
    }
  },
  "experience": {
    "title": "Expérience",
    "civibot": {
      "title": "Ingénieur Software",
      "company": "CiviBot",
      "duration": "Juillet 2025 — Aujourd’hui",
      "location": "Hybrid",
      "description": "J’ai intégré CiviBot, startup incubée à UC Berkeley SkyDeck, en tant qu’ingénieur full-stack chargé de faire évoluer la plateforme d’assistance IA destinée aux ingénieurs civils. Mon travail s’est articulé autour du développement d’un site React couplé à une API FastAPI, avec une forte exigence de qualité produit et de maintenabilité.\n\n Côté frontend, j’ai conçu et harmonisé plusieurs parcours utilisateurs (connexion, abonnement, compte utilisateur) tout en mettant en place une architecture d’internationalisation complète avec react-i18next. J’ai également refactorisé la navigation et le système de formulaires pour renforcer la cohérence UX et préparer l’industrialisation.\n\n  Côté backend, j’ai développé et documenté plusieurs endpoints critiques (gestion de compte, contact, récupération et changement de mot de passe), tout en participant à la mise en place de la fonctionnalité ChatPDF, intégrant un pipeline d’ingestion et d’indexation de documents avec génération de réponses contextualisées.\n\n  Depuis la fin du stage, je continue ma collaboration à distance avec l'équipe de CiviBot afin d’accompagner la montée en échelle technique du produit et son intégration continue."
    },
    "waterid": {
      "title": "Testeur qualité",
      "company": "Water ID",
      "duration": "Juillet 2021",
      "location": "Karlsruhe, Allemagne",
      "description": "Premier stage à l’étranger et première expérience en ingénierie. Participation aux tests d’étanchéité et de fiabilité de dispositifs électroniques destinés à l’analyse de la qualité de l’eau des piscines. Implication dans la validation fonctionnelle des prototypes et l’amélioration des procédures de test en environnement industriel."
    }
  },
  "projects": {
    "title": "Projets",
    "civibot": {
      "title": "CiviBot",
      "description": "<strong>CiviBot – Projet d’ingénierie Full Stack</strong>\n\nCiviBot est une startup basée à Berkeley, incubée au <strong>UC Berkeley SkyDeck</strong>, développant un assistant IA aidant les ingénieurs à accéder et interpréter les normes techniques. La plateforme combine une architecture web full-stack (React + FastAPI) avec un pipeline de traitement documentaire exploitant des modèles de langage naturel pour des questions-réponses contextuelles sur des PDF importés.\n\nDans ce projet, j’ai contribué à l’ensemble de la stack logicielle — du développement frontend (UI/UX) au backend et à la gestion des données IA — afin de livrer une expérience cohérente et prête pour la production.\n\n<strong>Frontend (React + Ant Design + i18next)</strong>\n\nJ’ai refondu et enrichi l’interface web React en utilisant les composants Ant Design pour garantir cohérence et modularité. J’ai implémenté une navigation dynamique selon l’état d’authentification (« Connexion/Inscription » → « Mon Compte/Déconnexion »), uniformisé le pied de page sur l’ensemble du site et développé les pages <em>À propos, Contact, Tarifs, Mon Compte et Abonnement.</em>  \nLe formulaire de contact intègre des règles de validation et des structures prêtes pour le backend. J’ai également ajouté une procédure de récupération de mot de passe (« Mot de passe oublié ») et un parcours d’abonnement avec tarification par paliers.  \nJ’ai mis en place l’internationalisation complète (FR/EN) avec `react-i18next`, en externalisant tous les textes d’interface dans des fichiers JSON pour un support multilingue évolutif. Ce choix architectural assure cohérence, maintenabilité et extensibilité linguistique future.\n\n<strong>Backend (FastAPI + PostgreSQL + Redis + GCS)</strong>\n\nCôté backend, j’ai conçu des endpoints REST avec validation stricte des données (schémas Pydantic), gestion robuste des erreurs et séparation claire entre routes, schémas et couche CRUD.  \nJ’ai développé des endpoints pour :  \n\n- L’envoi du formulaire de contact (POST validé + envoi d’email)  \n- La mise à jour sécurisée du mot de passe (vérification du hash + mise à jour en base)  \n- Les sessions de chat persistantes liées aux documents (ChatPDF)\n\n<strong>La fonctionnalité ChatPDF</strong> permet à l’utilisateur d’importer un PDF ou une image et de l’interroger en langage naturel. Chaque réponse est fondée sur le document d’origine et renvoie précisément aux passages sources. Techniquement, le pipeline inclut :  \n\n- Téléversement et stockage sur Google Cloud Storage (GCS)  \n- Enregistrement du document dans PostgreSQL (métadonnées, URL GCS)  \n- Persistance du chat (sessions, messages, lien PDF)  \n- Limitation de débit via Redis pour éviter les abus  \n- Reconstruction du contexte pour les conversations multi-tours\n\nDans le pipeline de traitement documentaire IA, j’ai contribué à la première étape du flux d’ingestion (import → analyse → vectorisation → indexation). Ce pipeline garantit que chaque requête renvoie des réponses traçables et fondées sur la source.\n\nCe projet a renforcé mon expertise full-stack, approfondi ma compréhension de la conception d’API et m’a appris à aligner UX, ingénierie et architecture de données dans un environnement startup dynamique.\n\n<strong>Compétences techniques principales :</strong> <em>React, Ant Design, FastAPI, PostgreSQL, embeddings, recherche vectorielle, i18n</em>\n\n"
    },
    "othello": {
      "title": "Othello IA (Navigateur)",
      "description": "<strong>Othello IA – Jeu de plateau vocal et intelligent</strong>\n\nCe projet académique combine la <strong>logique d’intelligence artificielle</strong> et une <strong>interface web moderne</strong> pour créer un jeu d’Othello interactif, jouable en ligne et par commande vocale.\n\n<strong>Interface (React)</strong>\n\nDéveloppée avec React, l’interface propose un design fluide et réactif, enrichi de thèmes saisonniers pour une expérience immersive (🌿❄🌸☀). Elle garantit cohérence visuelle et interactions intuitives sur toutes les plateformes.\n\n<strong>Système d’IA</strong>\n\nDeux modèles d’IA ont été intégrés pour offrir une stratégie évoluée :\n\n- <em>IA Classique (Algorithme Minimax)</em> : une approche déterministe simulant plusieurs coups à l’avance pour maximiser son avantage tout en limitant celui de l’adversaire.\n\n- <em>IA Apprentissage automatique (Random Forest Classifier)</em> : entraînée sur des données réelles pour prédire les meilleurs coups et s’améliorer en continu par auto-apprentissage.\n\nCette approche hybride associe stratégie déterministe et intelligence adaptative pour une comparaison de performance équilibrée.\n\n<strong>Reconnaissance vocale</strong>\n\nGrâce à <em>Python</em> et à l’<em>API Google Speech Recognition</em>, le système traite les commandes vocales via un filtre passe-bande isolant les fréquences de la parole et réduisant le bruit. Les commandes reconnues sont ensuite transmises au backend pour exécution.\n\n"
    },
    "sightsense": {
      "title": "Sightsense",
      "description": "<strong>SIGHT Senses – Aide à la navigation portable pour malvoyants</strong>\n\nSIGHT Senses est un projet académique d’ingénierie visant à concevoir un <strong>système de guidage portable</strong> destiné aux personnes malvoyantes. Le dispositif intègre <strong>vision par ordinateur</strong>, <strong>audio spatial</strong> et <strong>retour haptique</strong> dans une architecture unifiée basée sur un <em>Raspberry Pi</em>, capable de traiter en temps réel les informations de l’environnement pour offrir une aide à la navigation intuitive et multisensorielle.\n\n<strong>Vue d’ensemble du système</strong>\n\nLe cœur du système repose sur le Raspberry Pi, qui synchronise trois modules techniques : <em>traitement d’image</em>, <em>spatialisation sonore</em> et <em>retour haptique</em>. Chaque module fonctionne de manière autonome tout en échangeant des données en continu pour garantir une réactivité immédiate. Un script de démarrage (<code>crontab</code>) lance automatiquement le programme, rendant le dispositif entièrement autonome.\n\n<strong>Traitement d’image (Python + OpenCV)</strong>\n\nLe module de vision capture les images de la caméra et réalise la <strong>détection en temps réel des formes et obstacles</strong>. À l’aide d’OpenCV, il applique des masques de couleur (<code>cv2.inRange()</code>), un flou gaussien pour réduire le bruit, puis détecte les contours (<code>cv2.findContours()</code>). Les <em>carrés</em> et <em>croix</em> sont identifiés via une approximation polygonale (<code>cv2.approxPolyDP()</code>) et un filtrage géométrique basé sur la convexité et le ratio d’aspect. Les lignes bleues sont également détectées pour suivre la trajectoire. Seules les formes les plus pertinentes (plus grandes ou centrées) sont conservées. Les optimisations vectorielles d’OpenCV assurent un traitement fluide et rapide.\n\n<strong>Détection de distance et d’obstacles (Lidar)</strong>\n\nUn capteur <strong>TFLuna Lidar</strong> mesure en continu la distance grâce à une communication série (<code>/dev/serial0</code>). Les variations détectées indiquent la présence d’obstacles et leur proximité. Des seuils dynamiques traduisent la distance en intensité de retour sensoriel, coordonnant les réponses haptiques et sonores. Cette intégration permet une perception fiable des obstacles, même en faible luminosité.\n\n<strong>Traitement audio (Python + PyDub)</strong>\n\nLe <strong>système audio</strong> convertit les données spatiales en signaux directionnels. Conçu avec PyDub, il ajuste le volume et le délai entre les canaux stéréo pour créer une perception 3D réaliste. La fonction <code>balance_sound()</code> module le gain selon la position de l’obstacle. Chaque type d’objet détecté déclenche un son distinct préchargé (carré, croix, ligne, barrière), dont le volume varie avec la distance pour fournir un guidage clair et intuitif.\n\n<strong>Retour haptique (Python + RPi.GPIO)</strong>\n\nLe <strong>module haptique</strong> complète les signaux auditifs par des vibrations dont l’intensité dépend de la distance. Piloté par PWM via les broches GPIO du Raspberry Pi, les fonctions <code>vibrate()</code> et <code>haptic_()</code> produisent des vibrations continues, fortes ou saccadées selon la proximité de l’obstacle. Cette redondance sensorielle renforce la fiabilité du guidage dans tous les environnements.\n\n<strong>Intégration matérielle et modélisation 3D</strong>\n\nLe boîtier, modélisé sous SolidWorks, regroupe la caméra, le Raspberry Pi, le Lidar, la batterie et les moteurs vibrants dans une unité compacte fixée sur la poitrine. Des ouvertures assurent la ventilation et des sangles réglables garantissent confort et stabilité. La conception modulaire facilite la maintenance et l’évolution du prototype.\n\nCe projet illustre l’intégration harmonieuse de la <strong>vision par ordinateur</strong>, du <strong>traitement du signal</strong> et des <strong>systèmes embarqués</strong> au sein d’un dispositif portable et intelligent. En synchronisant la vision, le son et le toucher en temps réel, SIGHT Senses offre une aide à la navigation intuitive, robuste et accessible aux personnes malvoyantes."
    },
    "ui": {
      "previous": "Projet précédent",
      "next": "Projet suivant",
      "close": "Fermer",
      "previous_image": "Image précédente",
      "next_image": "Image suivante",
      "visit_site": "Consulter le site",
      "view_repo": "Voir le repository"
    }
  },
  "skills": {
    "title": "Compétences"
  }
}

